# -*- coding: utf-8 -*-
"""day16_activation_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BEfQtf6bRXlsETZxdjf-DUB41L38JGfO

### ReLU Implementation - For Hidden Layers
Returns the highest weight of two values (0 and x)
"""

from matplotlib import pyplot
 
# rectified linear function
def rectified(x):
	return max(0.0, x)
 
# Example inputs for visualization
inputs = [x for x in range(-10, 10)]
outputs = [rectified(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()

"""### Sigmoid Implementation - For Hidden Layers
Takes in a value and returns a normalized value between 0 and 1
"""

from math import exp
from matplotlib import pyplot
 
# sigmoid activation function
def sigmoid(x):
	return 1.0 / (1.0 + exp(-x))
 
# Example imputs for visualization
inputs = [x for x in range(-10, 10)]
outputs = [sigmoid(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()

"""### Tanh Implementation - For Hidden Layers
Takes in a value and returns a normalized value between -1 and 1
"""

from math import exp
from matplotlib import pyplot
 
# tanh activation function
def tanh(x):
	return (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 
# Example inputs for visualization
inputs = [x for x in range(-10, 10)]
outputs = [tanh(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()

"""### Linear Implementation - For Output Layers
Takes in a value and returns the value unchanged
"""

# example plot for the linear activation function
from matplotlib import pyplot

# linear activation function
def linear(x):
	return x

# Example inputs for visualization
inputs = [x for x in range(-10, 10)]
outputs = [linear(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()

"""### Softmax Implementation - For Output Layers
Takes in a value and returns a vector that can be summed to 1.0 to show the probabilities of a class
"""

from numpy import exp

# softmax activation function
def softmax(x):
	return exp(x) / exp(x).sum()

# Example inputs for visualization
inputs = [1.0, 3.0, 2.0]
outputs = softmax(inputs)
# report the probabilities
print(f"Vectors: {outputs}")
# report the sum of the probabilities
print(f"Vector sum: {outputs.sum()}")